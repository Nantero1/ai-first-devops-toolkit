---
description: This lessons-learned file serves as a critical knowledge base for capturing and preventing mistakes. During development, document any reusable solutions, bug fixes, or important patterns. Consult it before implementing any solution.
alwaysApply: false
---
*This lessons-learned file serves as a critical knowledge base for capturing and preventing mistakes. During development, document any reusable solutions, bug fixes, or important patterns using the format: [Timestamp] Category: Issue → Solution → Impact. Entries must be categorized by priority (Critical/Important/Enhancement) and include clear problem statements, solutions, prevention steps, and code examples. Only update upon user request with "lesson" trigger word. Focus on high-impact, reusable lessons that improve code quality, prevent common errors, and establish best practices. Cross-reference with .cursor\memories.md for context.*

# Lessons Learned

*Note: This file is updated only upon user request and focuses on capturing important, reusable lessons learned during development. Each entry follows format: [Timestamp] Priority: Category → Issue: [Problem] → Fix: [Solution] → Why: [Impact]. Use grep/automated tools to verify changes and prevent regressions.*

## 2025-01-06 - Code Quality and Refactoring

[2025-01-06 23:16] **Critical**: Code Smell Detection and Library Usage → Issue: Manual implementation of JSON schema to Pydantic model conversion instead of using dedicated libraries creates 150+ lines of maintenance burden and incomplete feature coverage → Fix: Replace manual implementation with json-schema-to-pydantic library, use multiple inheritance to maintain KernelBaseModel functionality → Why: Eliminates reinventing-the-wheel anti-pattern, reduces code by 150+ lines, improves robustness with comprehensive JSON schema support including references and combiners, follows DRY principle and reduces maintenance burden. Libraries exist for common problems - use them.

```python
# ❌ CODE SMELL - Manual implementation
def _convert_json_schema_field(field_schema: Dict[str, Any], field_name: str) -> tuple:
    # 150+ lines of manual type mapping...
    if field_type == "string":
        if "enum" in field_schema:
            # Manual enum handling...
    # More manual mapping...

# ✅ PROPER - Use dedicated library
from json_schema_to_pydantic import create_model as create_model_from_schema

def create_dynamic_model_from_schema(schema_dict: Dict[str, Any], model_name: str) -> Type[KernelBaseModel]:
    BaseGeneratedModel = create_model_from_schema(schema_dict)
    
    class DynamicKernelModel(KernelBaseModel, BaseGeneratedModel):
        pass
    
    return DynamicKernelModel
```

## 2025-01-06 - Schema Enforcement Implementation

[2025-01-06 22:48] **Critical**: Semantic Kernel ChatHistory Integration → Issue: Using kernel.invoke_prompt() with {{$chat_history}} template variable causes "Variable not found" error when chat_history is passed separately → Fix: Use service.get_chat_message_contents() directly with chat_history parameter instead of prompt templates → Why: Direct service call properly handles ChatHistory objects and avoids template variable resolution issues. Critical for structured output workflows.

```python
# ❌ WRONG - Template variable approach fails
result = await kernel.invoke_prompt(
    prompt="{{$chat_history}}",
    arguments=args,
    chat_history=chat_history,
)

# ✅ CORRECT - Direct service call works
result = await service.get_chat_message_contents(
    chat_history=chat_history,
    settings=settings,
    arguments=args,
)
```

[2025-01-06 22:47] **Critical**: Semantic Kernel Response Extraction → Issue: Different response objects returned by kernel.invoke_prompt() vs service.get_chat_message_contents() causing extraction failures → Fix: Handle both list[ChatMessageContent] and FunctionResult.value patterns with proper type checking → Why: Ensures robust response handling across different Semantic Kernel execution paths. Essential for production reliability.

```python
# ✅ ROBUST - Handle both response types
if isinstance(result, list) and len(result) > 0:
    # Direct service call returns list of ChatMessageContent
    response = result[0].content if hasattr(result[0], "content") else str(result[0])
elif hasattr(result, "value") and result.value:
    # Kernel invoke_prompt returns FunctionResult with value
    if isinstance(result.value, list) and len(result.value) > 0:
        response = result.value[0].content if hasattr(result.value[0], "content") else str(result.value[0])
    else:
        response = str(result.value)
else:
    response = str(result)
```

[2025-01-06 22:45] **Important**: Dynamic Pydantic Model Creation → Issue: JSON schemas need runtime conversion to KernelBaseModel classes for 100% enforcement → Fix: Use pydantic.create_model() with custom field mapping function to convert JSON schema properties to Pydantic field definitions → Why: Enables true token-level constraint enforcement through settings.response_format = ModelClass. Achieves 100% schema compliance vs basic JSON mode.

```python
# ✅ PATTERN - Dynamic model creation
def create_dynamic_model_from_schema(schema_dict: Dict[str, Any], model_name: str) -> Type[KernelBaseModel]:
    properties = schema_dict.get("properties", {})
    required_fields = schema_dict.get("required", [])
    field_definitions = {}
    
    for field_name, field_schema in properties.items():
        field_type, field_info = _convert_json_schema_field(field_schema, field_name)
        if field_name in required_fields:
            field_definitions[field_name] = (field_type, field_info)
        else:
            field_definitions[field_name] = (Optional[field_type], Field(default=None, **field_info.extra))
    
    return create_model(model_name, __base__=KernelBaseModel, **field_definitions)
```

[2025-01-06 22:43] **Important**: JSON Schema Field Type Mapping → Issue: JSON schema types (string, number, array, etc.) need conversion to Python types with constraint validation → Fix: Create comprehensive mapping function handling enums, numeric ranges, array limits, string constraints → Why: Ensures all JSON schema validation rules are enforced at the Pydantic model level. Critical for maintaining schema integrity.

```python
# ✅ COMPREHENSIVE - Handle all JSON schema types
def _convert_json_schema_field(field_schema: Dict[str, Any], field_name: str) -> tuple:
    field_type = field_schema.get("type")
    field_kwargs = {"description": field_schema.get("description", "")} if field_schema.get("description") else {}
    
    if field_type == "string":
        python_type = str
        if "enum" in field_schema:
            enum_values = field_schema["enum"]
            field_kwargs["pattern"] = f"^({'|'.join(enum_values)})$"
        if "maxLength" in field_schema:
            field_kwargs["max_length"] = field_schema["maxLength"]
        if "minLength" in field_schema:
            field_kwargs["min_length"] = field_schema["minLength"]
    elif field_type == "number":
        python_type = float
        if "minimum" in field_schema:
            field_kwargs["ge"] = field_schema["minimum"]
        if "maximum" in field_schema:
            field_kwargs["le"] = field_schema["maximum"]
    # ... handle other types
    
    return python_type, Field(**field_kwargs)
```

[2025-01-06 22:42] **Enhancement**: Structured Output Enforcement Architecture → Issue: Basic JSON mode ({type: "json_object"}) provides no schema validation → Fix: Use settings.response_format = KernelBaseModel for token-level constraint enforcement → Why: Achieves 100% schema compliance through Azure OpenAI's structured outputs feature. Eliminates schema validation errors in production CI/CD pipelines.

```python
# ❌ WEAK - Basic JSON mode (no enforcement)
settings.response_format = {"type": "json_object"}

# ✅ STRONG - Token-level constraint enforcement
settings.response_format = DynamicPydanticModel  # 100% guaranteed compliance
```

# ✅ STRONG - Token-level constraint enforcement
settings.response_format = DynamicPydanticModel  # 100% guaranteed compliance
```


## 2025-01-06 - Comprehensive Testing Infrastructure

[2025-01-06 23:45] **Critical**: Python Metaclass Compatibility Issues → Issue: Using Mock objects in tests for KernelBaseModel-based classes causes TypeError due to metaclass conflicts between Mock and BaseModel metaclasses → Fix: Replace Mock objects with actual Pydantic BaseModel classes in test fixtures, use @patch decorators with proper BaseModel subclasses → Why: Eliminates metaclass conflicts while maintaining test isolation. Critical for testing schema enforcement functionality with KernelBaseModel inheritance. Affected 7 of 18 test failures.

```python
# ❌ WRONG - Mock objects cause metaclass conflicts
@patch("llm_runner.create_dynamic_model_from_schema")
def test_schema_function(mock_create_model):
    mock_create_model.return_value = Mock()  # Fails with metaclass conflict
    
# ✅ CORRECT - Use actual BaseModel classes
@patch("llm_runner.create_dynamic_model_from_schema")
def test_schema_function(mock_create_model):
    class MockModel(BaseModel):
        test_field: str = "test"
    mock_create_model.return_value = MockModel
```

[2025-01-06 23:42] **Important**: Realistic Mocking Strategy → Issue: Synthetic mocks fail to capture actual API response complexity leading to false test confidence → Fix: Generate mocks from actual API responses using debug mode, create mock_factory.py with realistic ChatMessageContent structure including inner_content, metadata, and usage statistics → Why: Ensures test mocks match production behavior, catches integration issues early. Based on captured Azure OpenAI API responses with proper structure.

```python
# ❌ SYNTHETIC - Oversimplified mocks
def create_mock_response():
    return Mock(content="simple response")

# ✅ REALISTIC - Based on actual API responses
def create_chat_message_content_mock(content: str = "Test response") -> ChatMessageContent:
    return ChatMessageContent(
        role="assistant",
        content=content,
        inner_content=TextContent(text=content),
        metadata={"usage": {"total_tokens": 150}},
        model_id="gpt-4"
    )
```

[2025-01-06 23:40] **Important**: Systematic Test Failure Resolution → Issue: 18 failing tests across multiple categories (imports, mocking, business logic) overwhelm debugging efforts → Fix: Categorize failures by complexity (Easy: 4 tests, Medium: 7 tests, Complex: 7 tests), tackle systematically starting with easiest wins → Why: Achieves 100% test pass rate through organized approach, prevents debugging fatigue, ensures no failures are missed. Methodology scales to larger codebases.

```python
# ✅ SYSTEMATIC APPROACH - Categorize and prioritize
"""
Easy Fixes (4 tests):
- Logger assertion issues
- Mock setup problems  
- Simple return value mismatches

Medium Fixes (7 tests):
- Exception type alignment
- Error message regex patterns
- Import statement updates

Complex Fixes (7 tests):
- Metaclass compatibility
- Schema model mocking
- Retry logic testing
"""
```

[2025-01-06 23:38] **Important**: Test Architecture Design → Issue: Monolithic test structure lacks proper separation of concerns and makes maintenance difficult → Fix: Implement three-tier architecture: tests/unit/ (heavy mocking), tests/integration/ (minimal mocking), acceptance/ (real API calls with LLM-as-judge) → Why: Provides comprehensive coverage at different levels, enables faster unit tests while maintaining end-to-end validation. Follows industry best practices for test pyramid.

```python
# ✅ THREE-TIER ARCHITECTURE
tests/
├── unit/                     # 69 tests, heavy mocking, fast execution
│   ├── test_schema_functions.py
│   ├── test_semantic_kernel_functions.py
│   └── test_input_output_functions.py
├── integration/              # End-to-end with realistic mocks
│   ├── test_examples_integration.py
│   └── test_cli_interface.py
└── acceptance/               # Real API calls, quality evaluation
    └── llm_as_judge_acceptance_test.py
```

[2025-01-06 23:36] **Enhancement**: Exception Handling Alignment → Issue: Test expectations don't match actual function behavior causing false failures → Fix: Analyze actual exception types and messages thrown by functions, update test assertions to match reality → Why: Ensures tests validate actual behavior rather than assumed behavior. Prevents brittle tests that break when error messages change.

```python
# ❌ ASSUMED - Test expects wrong exception type
def test_invalid_json_schema():
    with pytest.raises(SchemaValidationError):  # Wrong exception type
        load_json_schema("invalid.json")

# ✅ ALIGNED - Test matches actual behavior  
def test_invalid_json_schema():
    with pytest.raises(InputValidationError):  # Actual exception type
        load_json_schema("invalid.json")
```
